{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Reference Layers\n",
    "\n",
    "A reference made for personal use on common layers in PyTorch. Not meant to be comprehensive. \n",
    "\n",
    "Currently:\n",
    "\n",
    "1. Linear\n",
    "2. Embeddings\n",
    "3. Dropout\n",
    "4. Transformers\n",
    "\n",
    "Created by Josiah Davis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform Linux-4.15.0-1060-aws-x86_64-with-debian-buster-sid\n",
      "Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n",
      "PyTorch 1.3.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(\"Platform\", platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import torch; print(\"PyTorch\", torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear\n",
    "\n",
    "Handles the core matrix multiply of the form $y = xA^T + b$, where $x$ is the data, $A$ is the learned weight parameter and $b$ is the bias term.\n",
    "\n",
    "### [`nn.Linear(in_features, out_features)`](https://pytorch.org/docs/stable/nn.html#linear)\n",
    "\n",
    "- **Input**: 2D,3D,4D of the form [observations [, n_heads, seq_len,] in_features] (e.g., [10, 8]).\n",
    "- **Arguments**: First arg is number of columns in the input called `in_features` (e.g., 8), second arg is number of columns in output called `out_features` (e.g., 16).\n",
    "- **Output**: 2D,3D,4D of the form [observations [, n_heads, seq_len,] out_features] (e.g., [10, 16])\n",
    "- **Stores**: Layer stores two parameters, the bias with shape=`out_features` and weight with shape=[`out_features`, `in_features`]). The weight matrix is tranposed before being multiplied by the input matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10, 8) # e.g., [observations, hidden]\n",
    "lin = nn.Linear(8, 16)\n",
    "y = lin(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D Example, e.g., language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 16])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10,3, 8) # e.g., [observations, seq_len, hidden]\n",
    "y = lin(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4D example, e.g., transformer self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 3, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10, 2, 3, 8) # e.g., [observations, n_heads, seq_len, hidden_per_head]\n",
    "y = lin(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.bias.shape # [out_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.weight.shape # [out_features, in_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values are initialized by default with uniform distribution about $\\sqrt{\\frac{1}{{in\\_features}}}$ ([source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear)). \n",
    "\n",
    "For example, we had 8 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3535533905932738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.sqrt(1/8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.4990752e-01 -2.7985743e-01 -2.0980732e-01 -1.3975722e-01\n",
      " -6.9707118e-02  3.4298003e-04  7.0393078e-02  1.4044318e-01\n",
      "  2.1049328e-01  2.8054339e-01  3.5059348e-01]\n"
     ]
    }
   ],
   "source": [
    "vals = lin.weight.detach().numpy()\n",
    "counts, cutoffs = np.histogram(vals)\n",
    "print(cutoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization could be changed as desired. For example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLin(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLin, self).__init__()\n",
    "        self.lin1 = nn.Linear(8, 16)\n",
    "        self.lin2 = nn.Linear(16, 6)\n",
    "        self._init_layers()\n",
    "        \n",
    "    def _init_layers(self):\n",
    "        init_1 = np.sqrt(6) / np.sqrt(8 + 16)\n",
    "        self.lin1.weight.data.uniform_(-init_1, init_1)\n",
    "        self.lin1.bias.data.zero_()\n",
    "        \n",
    "        self.lin2.weight.data.normal_(mean=0, std=np.sqrt(6 / 16))\n",
    "        self.lin2.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin2(self.lin1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = CustomLin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[-0.49227554 -0.39338982 -0.29450414 -0.19561842 -0.09673272  0.00215298\n",
      "  0.10103868  0.19992438  0.2988101   0.39769578  0.4965815 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(6) / np.sqrt(8 + 16))\n",
    "counts, cutoffs = np.histogram(cl.lin1.weight.detach().numpy())\n",
    "print(cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6123724356957945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.055361047, 0.65983176)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sqrt(6 / 16))\n",
    "wts = cl.lin2.weight.detach().numpy()\n",
    "wts.mean(), wts.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning about some of the history and development of weight initialization, consult:\n",
    "- [Understanding the difficulty of training deep feedforward neural networks (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) by Xavier Glorot and Yoshua Bengio\n",
    "- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (2015)](https://arxiv.org/pdf/1502.01852.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren,  Jian Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "### [`nn.Embedding(num_embeddings, embedding_dim)`](https://pytorch.org/docs/stable/nn.html#embedding)\n",
    "\n",
    "Simple lookup of any categorical variable (often NLP vocabulary) mapping to dense floating point representations.\n",
    "\n",
    "#### Shape\n",
    "\n",
    "- Input: Could be 1D or 2D depending on the application.\n",
    "- Output: If the input is 1D, output will be 2D, if the input is 2D, output will be 2D.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **num_embeddings** (int) – how large is the vocab in dictionary or how many categories are there? (e.g., 25)\n",
    "- **embedding_dim** (int) – the number of features (e.g., 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D input example (e.g., a single column in tablular data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([3])\n",
      "output shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([15, 20, 7])\n",
    "print(f'input shape: {x.shape}')\n",
    "emb = nn.Embedding(num_embeddings=25, embedding_dim=5)\n",
    "y = emb(x)\n",
    "print(f'output shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D input example (e.g., text for language modelling with sequence on the rows and \"text chunk\" on the columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 3])\n",
      "output shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[15, 20, 7],[23, 10, 6]])\n",
    "print(f'input shape: {x.shape}')\n",
    "emb = nn.Embedding(num_embeddings=25, embedding_dim=5)\n",
    "y = emb(x)\n",
    "print(f'output shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0860,  1.2532,  0.0364, -0.4219,  0.7459],\n",
       "        [ 3.5131, -0.4061,  0.3684,  0.6196, -0.9366],\n",
       "        [ 0.4579, -2.9096, -0.1020,  1.9300,  0.6954],\n",
       "        [-0.9681, -0.6774,  0.2202,  1.0518,  0.9375],\n",
       "        [-1.6973,  1.3657, -0.1358,  0.1784,  0.8525],\n",
       "        [ 0.4791, -0.6650,  1.3976, -2.5129, -0.0492],\n",
       "        [-0.1043, -0.8984, -1.5367, -1.1828,  1.3752],\n",
       "        [ 0.9068, -0.0710, -1.5567,  0.8874,  0.1792],\n",
       "        [-1.0142,  1.0876, -1.7989, -0.9655, -0.2474],\n",
       "        [-1.4501, -1.2906,  0.5528,  3.1640, -1.8497],\n",
       "        [ 0.2514, -0.4580,  0.4639,  0.1454, -0.4032],\n",
       "        [-1.1059,  0.4435,  1.2726, -0.0141, -0.0754],\n",
       "        [ 0.0366, -0.9740,  0.5161,  1.2957,  0.7978],\n",
       "        [-0.3188, -0.9667, -0.8374,  0.3967,  0.3336],\n",
       "        [ 0.7357, -1.3129, -2.5905, -0.1475,  0.5877],\n",
       "        [-0.5707,  0.0450, -0.2404,  0.6378, -0.9729],\n",
       "        [ 1.4942, -0.5200,  1.0599, -0.2466, -1.0081],\n",
       "        [ 0.2316,  0.4852,  0.7389,  0.4103, -1.6214],\n",
       "        [-1.8131, -1.5694,  0.0185, -0.0139,  0.6958],\n",
       "        [-0.4270, -1.6377, -0.8169, -0.0999, -0.3422],\n",
       "        [-1.8016,  1.7424, -2.4283, -0.3420,  0.1752],\n",
       "        [-0.6404, -1.1784, -0.1709,  0.4407, -2.0062],\n",
       "        [-0.1921, -0.0372,  1.6190, -0.5005, -0.8779],\n",
       "        [ 0.4825,  1.2597, -0.5964, -0.3184, -0.8306],\n",
       "        [ 1.2492,  1.3107, -0.8360, -0.7884,  0.5134]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropout\n",
    "\n",
    "### [`nn.Dropout(p)`](https://pytorch.org/docs/master/nn.html#dropout)\n",
    "\n",
    "Randomly zero out elements from input matrix for regularization. Doesn't work when `model.eval()` is set. Also re-normalizes the output values not zero'd out by $\\frac{1}{1-p}$.\n",
    "\n",
    "**Input:** Can be any shape\n",
    "\n",
    "**Output:** Same shape as input\n",
    "\n",
    "**See Also:** `nn.Dropout2d` and `nn.Dropout3d` for zero-ing entire channels at a time.\n",
    "\n",
    "**Paper**: Improving neural networks by preventing co-adaptation of feature detectors (2012) ([arxiv](https://arxiv.org/pdf/1207.0580.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 4])\n",
      "output shape: torch.Size([2, 4])\n",
      "output:\n",
      "tensor([[ 4.3167,  0.0000,  0.0000, -0.8370],\n",
      "        [-1.0616, -0.0000,  0.0000, -1.3493]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 4)\n",
    "print(f'input shape: {x.shape}')\n",
    "do = nn.Dropout(p=0.6)\n",
    "y = do(x)\n",
    "print(f'output shape: {y.shape}')\n",
    "print(f'output:\\n{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is only activated once the training mode is turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.do = nn.Dropout(0.6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.do(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3708, -0.2339,  0.5004, -0.7387],\n",
       "        [-1.8816, -0.2013, -0.8256, -0.7826]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running with `model.train()`. This will give you a different output each time you call it (unless you set a seed). \n",
    "\n",
    "Incidentally, this is the key to monte carlo dropout, a technique for uncertainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "tensor([[ 0.9271, -0.5847,  0.0000, -1.8468],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "model = DOModel()\n",
    "model.train()\n",
    "y = model(x)\n",
    "print(f'output:\\n{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running again with `model.eval()`. This will give you the same output no matter how many times you call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      "tensor([[ 0.3708, -0.2339,  0.5004, -0.7387],\n",
      "        [-1.8816, -0.2013, -0.8256, -0.7826]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y = model(x)\n",
    "print(f'output:\\n{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformers\n",
    "\n",
    "### [`nn.TransformerEncoderLayer(d_model, nhead)`](https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer)\n",
    "\n",
    "Transformer operations are defined in \"Attention is all you need (2017).\" Differece between this and the Decoder layer is that the Encoder only attends to itself (Key/Value/Query is the source langage). Whereas in Decoder layer there is an attention over the memory (i.e., encoding of the input sequence) as well as the self-attention over the target sequence.\n",
    "\n",
    "**Input**: A 3D input of the structure [sequence length, batch size, hidden features].\n",
    "\n",
    "**Output**: Same structure as input.\n",
    "\n",
    "**Operation**: Key, Value, and Query are all the source sentence (only self-attention). See [source code](https://pytorch.org/docs/master/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer)\n",
    "\n",
    "```\n",
    "src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "src = src + self.dropout1(src2)\n",
    "src = self.norm1(src)\n",
    "src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "src = src + self.dropout2(src2)\n",
    "src = self.norm2(src)\n",
    "return src\n",
    "```\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- **d_model** – the number of expected features in the input (required).\n",
    "- **nhead** – the number of heads in the multiheadattention models (required).\n",
    "- **dim_feedforward** – the dimension of the feedforward network model (default=2048).\n",
    "- **dropout** – the dropout value (default=0.1).\n",
    "- **activation** – the activation function of intermediate layer, relu or gelu (default=relu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([10, 32, 512])\n",
      "output shape: torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10, 32, 512)\n",
    "print(f'input shape: {x.shape}')\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "y = encoder_layer(x)\n",
    "print(f'output shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization\n",
    "\n",
    "### [`nn.LayerNorm`](https://pytorch.org/docs/stable/nn.html#layernorm)\n",
    "\n",
    "Layer Normalization is described in the paper from 2016, [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf). It consists of subtracting the mean and dividing by the variance of the data coming into the layer. You get to choose what you want dimensions you would like to use when computing the mean and variance. Unlike Batch normalization, this layer conducts normalization during training and infernce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 5)\n",
    "# With Learnable Parameters\n",
    "m = nn.LayerNorm(5, elementwise_affine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9569,  0.2346, -0.1040, -1.5393, -1.0113],\n",
       "        [-0.0372,  1.7077, -2.5073, -0.8248,  0.7692],\n",
       "        [-0.3095,  0.7462,  0.1451,  1.7440, -0.3375]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.67539585"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_x = np.mean(x[0,:].detach().numpy())\n",
    "E_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41861182"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_x = np.var(x[0,:].detach().numpy())\n",
    "V_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4351)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[0,0] - E_x) / np.sqrt(V_x + m.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4351,  1.4065,  0.8831, -1.3353, -0.5192],\n",
       "        [ 0.0984,  1.3131, -1.6212, -0.4499,  0.6597],\n",
       "        [-0.9071,  0.4471, -0.3240,  1.7271, -0.9430]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
