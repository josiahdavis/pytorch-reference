{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Self Attention\n",
    "\n",
    "Here I create an annotated version of HuggingFace implementation of [BertSelfAttention Class](https://github.com/huggingface/transformers/blob/ab756f713c7dd5257e27bb74edd906dfdfbf0e5d/src/transformers/modeling_bert.py#L183).\n",
    "\n",
    "Note, this is different implementation than the one in PyTorch [MultHeadAttention](https://pytorch.org/docs/master/_modules/torch/nn/modules/activation.html#MultiheadAttention), although the quantiative results should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform Linux-4.15.0-1060-aws-x86_64-with-debian-buster-sid\n",
      "Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n",
      "PyTorch 1.3.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(\"Platform\", platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import torch; print(\"PyTorch\", torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation of HuggingFace Class\n",
    "\n",
    "I insert comments or ammend comments that were already there without altering any of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pytorch module\n",
    "class BertSelfAttention(nn.Module):\n",
    "    # initializing with configuration\n",
    "    def __init__(self, config):\n",
    "        # Initializing parent classes\n",
    "        super().__init__()\n",
    "        # Check that hidden size is divisible by # of attn heads\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "        # store # of output attentions from config\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        # store  # of attention heads ...\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        # store size for a single attention head\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        # Set size of all heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Create  query/key/value as linear layer (with an option to shrink of\n",
    "        # number of attention heads decreases)\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        # dropout layer with p from configuration\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    # transform a matrix of shape [batch_size, seq_len, hidden_size] to\n",
    "    # [batch_size, num_attn_heads, seq_len, hidden_size/num_att_heads]\n",
    "    # right before doing our dot product. This appears to be a \"trick\" \n",
    "    # to perform the matrix multiply on a per attention head basis.\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (\n",
    "            self.num_attention_heads,\n",
    "            self.attention_head_size,\n",
    "        )\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    # forward pass with hidden states\n",
    "    # attention mask and head mask\n",
    "    # also encoder hidden states and\n",
    "    # Encoder attention mask (?)\n",
    "    def forward(self,hidden_states,attention_mask=None,\n",
    "        head_mask=None, encoder_hidden_states=None,encoder_attention_mask=None):\n",
    "        # calculate outputs from query\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        if encoder_hidden_states is not None:\n",
    "            mixed_key_layer = self.key(encoder_hidden_states)\n",
    "            mixed_value_layer = self.value(encoder_hidden_states)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            # if not instantiated as a cross-attention module, the keys\n",
    "            # and values coem from the hidden states\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # we need to maniuplate the outputs from [batch_size, seq_len, hidden_size] to\n",
    "        # [batch_size, num_attn_heads, seq_len, hidden_size/num_att_heads] \n",
    "        # in order to perform our dot product aka similarity calculation\n",
    "        # for each attention head separately \n",
    "        # we do this for the query, key, and value outputs\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attn scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        # Divide attn scores by the square root of the attn head dimension\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        # Check whether attention mask is provided\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for \n",
    "            # all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        # This is actually dropping out entire tokens to attend to, \n",
    "        # which might seem a bit unusual, but is taken from \n",
    "        # the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        # Check whether head mask is provided\n",
    "        if head_mask is not None:\n",
    "            # Mask heads if we want to\n",
    "            attention_probs = attention_probs * head_mask\n",
    "            \n",
    "        # Standard matrix mult. between attention probabilities\n",
    "        # and the value layer outputs (call it context)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        \n",
    "        # Swap out the 1 and 2 dimensions so:\n",
    "        # [obs, # attn heads, seq_len, attn head dimension] to\n",
    "        # [obs, seq_len, # attn heads, attn head dimension]\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        # Calculate the new shape which would be\n",
    "        # [obs, seq_len, #attn heads * attn head dimension]\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        \n",
    "        # Reshape the tensor\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        \n",
    "        # Check whether we want to output the attentions\n",
    "        # if so, return a tuple of context output and attention layer\n",
    "        # if not, return a tuple of context output only\n",
    "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the inputs which we will use a slightly simplified BertConfig class from HuggingFace to do ([link](https://github.com/huggingface/transformers/blob/master/src/transformers/configuration_bert.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        output_attentions=False\n",
    "):\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.output_attentions = output_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(output_attentions=False)\n",
    "bsa = BertSelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x = torch.rand(32, config.max_position_embeddings, config.hidden_size)\n",
    "output_context = bsa(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 768])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_context.shape # e.g., obs, seq_len, hidden features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the `transpose_for_scores` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "num_attention_heads = 2\n",
    "attention_head_size = int(hidden_size / num_attention_heads)\n",
    "all_head_size = num_attention_heads * attention_head_size\n",
    "assert hidden_size % num_attention_heads == 0\n",
    "query = nn.Linear(hidden_size, all_head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 16])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seq_len = 10\n",
    "hidden_states = torch.rand(batch_size, seq_len, hidden_size)\n",
    "mixed_query_layer = query(hidden_states)\n",
    "mixed_query_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally the query output is of the form [batch_size, seq_len, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.size()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 16])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose function begins\n",
    "def transpose_for_scores(x):\n",
    "    # Prior shape is [batch_size, seq_len, hidden_size]\n",
    "    # Create tuple for new shape:\n",
    "    #    [batch_size, seq_len, num_attn_heads, attn_head_size]\n",
    "    # Essentially\n",
    "    new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "    # Change to new shape\n",
    "    x = x.view(*new_x_shape)\n",
    "    # swap seq_len and num_attn_heads dimensions\n",
    "    return x.permute(0, 2, 1, 3)\n",
    "    # Transpose function ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 10, 8])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_layer = transpose_for_scores(mixed_query_layer)\n",
    "query_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is split into its respective attention heads before the matrix multiply. Brilliant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiative Comparison to PyTorch Multhead Attention\n",
    "\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
